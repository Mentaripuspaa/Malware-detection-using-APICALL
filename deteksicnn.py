from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten
from keras.models import Model
from keras.models import model_from_json
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adadelta, RMSprop,SGD,Adam
from keras import backend as K
from sklearn.metrics import confusion_matrix, recall_score, classification_report, f1_score
import numpy as np
import pandas as pd
import io
import csv
from numpy import empty
from statistics import mean
from sklearn.model_selection import train_test_split
from keras.models import load_model
from keras.models import model_from_json
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D
from keras.models import Model
from keras.models import model_from_json
from keras.utils import to_categorical
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adadelta, RMSprop,SGD,Adam
from keras import backend as K
from keras.models import Model
from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten
import matplotlib.pyplot as plt
from keras import  losses
from keras.callbacks import ModelCheckpoint
from keras.callbacks import TensorBoard
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from numpy import argmax

# Reading data #
with open('data_train_test_225fitur.csv', 'r') as f:
    reader = csv.reader(f, delimiter=',')
    data = list(reader)
result = np.array(data[0:256]).astype("str")    #read data in string form
#print(result.shape)
feature_names = result[0, 0 : len(result[1])]
labels = result[1:, -1]                                      #label for classification

labelencoder = LabelEncoder()
int_encoder = labelencoder.fit_transform(labels)

onehot_encoder = OneHotEncoder(sparse=False)
int_encoder = int_encoder.reshape(len(int_encoder),1)
onehot_enc = onehot_encoder.fit_transform(int_encoder)
inverted = labelencoder.inverse_transform([argmax(onehot_enc[0,:])])
#print(labels)
#print(int_encoder)
#print(onehot_enc)
#print(inverted)

result_train = result[1: , 1:len(result[1])-2] #mengambil nilai seluruh matriks dengan mengabaikan label
result_train = np.array(result_train).astype(np.float)
train_x = np.empty((len(result_train),15,15,1)) #15 baris x 15 kolom x 1 layer (karena direkonstruksi ke matriks citra biner)
for j in range(0, len(result_train)):                         #reshaping to 2d array for convolutional autoencoder
  resultraintemp = np.reshape(np.pad(result_train[j], (0, 2), 'constant'),(15,15, 1))
  train_x[j] = resultraintemp

#ENCODER NODE
input_img = Input(shape=(15,15,1))
x = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv1')(input_img)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2), padding='same', name='max1')(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2), padding='same', name='max2')(x)
x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv3')(x)
x = BatchNormalization()(x)
encoded = MaxPooling2D((2, 2), padding='same', name='max3')(x)

#DECODER NODE
x = Conv2D(64, (4, 4), activation='relu', padding='same', name='conv4')(encoded)
x = BatchNormalization()(x)
x = UpSampling2D((1, 1), name='up1')(x)
x = Conv2D(32, (4, 4), activation='relu', padding='same', name='conv5')(x)
x = BatchNormalization()(x)
x = UpSampling2D((1, 1), name='up2')(x)
x = Conv2D(16, (4, 4), activation='relu', padding='same', name='conv6')(x)
x = BatchNormalization()(x)
x = UpSampling2D((7, 7),name='up3')(x)
decoded = Conv2D(1,(4,4), activation='sigmoid', padding='same', name='conv7')(x)
autoencoder = Model(input_img, decoded)
autoencoder.compile(Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), loss='binary_crossentropy', metrics=['accuracy'])
autoencoder = Model(input_img, decoded)
autoencoder.load_weights("modelAE.h5")
#print("Loaded Model from disk")
print(encoded[1])
autoencoder.layers[0].trainable = True
autoencoder.layers[1].trainable = True
autoencoder.layers[2].trainable = True
autoencoder.layers[3].trainable = True
autoencoder.layers[4].trainable = True
autoencoder.layers[5].trainable = True
autoencoder.layers[6].trainable = True
autoencoder.layers[7].trainable = True
autoencoder.layers[8].trainable = True
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.layers.pop()
autoencoder.compile(Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), loss='binary_crossentropy')
autoencoder.summary()

#CLASSIFIER LAYER
y = Conv2D(64, (3, 3), activation='relu', padding = 'same' , name = 'conv8')(encoded)
y = BatchNormalization()(y)
y = MaxPooling2D((2,2), padding='same', name='max4')(y)
y = Dropout(0.5)(y)
y = Conv2D(32, (3,3), activation='relu', padding='same', name= 'conv9')(y)
y = BatchNormalization()(y)
y = MaxPooling2D((2,2), padding='same', name='max5')(y)
y = Dropout(0.5)(y)
y = Conv2D(16, (3,3), activation='relu', padding='same', name='conv10')(y)
y = BatchNormalization()(y)
y = MaxPooling2D((2,2), padding='same', name='max6')(y)
y = Dropout(0.5)(y)
y = Flatten()(y)
y = Dense(100, activation='relu')(y) #hidden layer-1
y = Dropout(0.5)(y)
#y = Dense(100, activation='relu')(y) #hidden layer-1
#y = Dropout(0.5)(y)
y = Dense(50, activation='relu')(y)
y = Dropout(0.5)(y)
y = Dense(50, activation='relu')(y)
y = Dropout(0.5)(y)
#y = Dense(50, activation='relu')(y)
#y = Dropout(0.5)(y)
yA = Dense(2, activation='sigmoid')(y) #output layer
classi = Model(autoencoder.input, yA)

train_x, test_x, train_y, test_y = train_test_split(train_x,
                                                    onehot_enc,
                                                    test_size=55,
                                                    random_state=5)

classi.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])
classi.summary()
history = classi.fit(train_x,train_y,
                	 epochs=100,
               		 batch_size=8,
               		 shuffle=True,
               		 validation_data=(test_x, test_y),
                     verbose=1,
                    #callbacks=[checkpointer]).history
               		 callbacks=[TensorBoard(log_dir='/tmp/encoderdecoder')])
classi.predict(test_x)
test_eval = classi.evaluate(test_x, test_y, verbose=1)
predict_class = classi.predict(test_x)
predict_class = (predict_class >= 0.5)
#print(predict_class)
#print(classification_report(test_y,predict_class)) #macro avg for average f1-score each class, #weighted avg for compute average weight for each class
print('accuracy score : ',accuracy_score(test_y, predict_class)) #micro avg for compute true positive/false negative each class
print('test loss : ', test_eval[0]*100)
#print('test accuracy : ', test_eval[1]*100)
#print(confusion_matrix(test_y.argmax(axis=1), predict_class.argmax(axis=1)))
#print("Accuracy %.2f%% " % (mean(history.history['acc'])*100))
#print("Val Accuracy %.2f%% " %(mean(history.history['val_acc'])*100))